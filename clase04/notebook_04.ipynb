{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller Práctico Clase 4: SVM y el Poder de los Kernels\n",
    "\n",
    "**Objetivo:** En este notebook, aplicaremos los conceptos de Máquinas de Vectores de Soporte (SVM) para resolver un problema de clasificación no lineal. Compararemos el rendimiento de SVM con diferentes kernels frente a un modelo lineal como la Regresión Logística y visualizaremos sus fronteras de decisión para desarrollar una intuición profunda sobre su funcionamiento.\n",
    "\n",
    "**Temas a cubrir:**\n",
    "1.  Generación y visualización de un dataset no lineal.\n",
    "2.  Entrenamiento de un modelo de Regresión Logística como base.\n",
    "3.  Entrenamiento de SVM con kernel lineal, polinómico y RBF.\n",
    "4.  Visualización interactiva de las fronteras de decisión.\n",
    "5.  Análisis del impacto de los hiperparámetros `C` y `gamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación del Entorno e Importación de Librerías\n",
    "\n",
    "Primero, importaremos todas las librerías que necesitaremos para nuestro análisis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Manipulación de datos\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-Learn para datasets, modelos y preprocesamiento\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Plotly para visualizaciones interactivas\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generación y Exploración del Dataset\n",
    "\n",
    "Para demostrar el poder de los kernels, necesitamos un dataset que no sea linealmente separable. `make_moons` de Scikit-Learn es perfecto para esto, ya que genera dos \"medias lunas\" entrelazadas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generamos el dataset\n",
    "X, y = make_moons(n_samples=500, noise=0.25, random_state=42)\n",
    "\n",
    "# Dividimos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Escalamos las características. Es una buena práctica para SVM.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Visualicemos los datos de entrenamiento\n",
    "fig = px.scatter(x=X_train[:, 0], y=X_train[:, 1], color=y_train.astype(str),\n",
    "                 color_discrete_map={'0': 'blue', '1': 'red'},\n",
    "                 title='Dataset de Entrenamiento (Moons)',\n",
    "                 labels={'x': 'Característica 1', 'y': 'Característica 2'})\n",
    "fig.update_layout(showlegend=False, height=600, width=800)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparación de Modelos y Fronteras de Decisión\n",
    "\n",
    "Ahora vamos a entrenar nuestros modelos y, para cada uno, visualizaremos la frontera de decisión que aprende. Para ello, crearemos una función auxiliar que nos facilite el ploteo."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_decision_boundary(model, X, y, title):\n",
    "    \"\"\"Función para visualizar la frontera de decisión de un modelo de clasificación en 2D.\"\"\"\n",
    "    # Creamos una malla de puntos para evaluar el modelo\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    # Predecimos sobre cada punto de la malla\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Creamos la figura\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Añadimos la superficie de decisión (contour plot)\n",
    "    fig.add_trace(go.Contour(\n",
    "        x=xx[0], y=yy[:, 0],\n",
    "        z=Z, showscale=False,\n",
    "        colorscale=['rgba(0,0,255,0.2)', 'rgba(255,0,0,0.2)'],\n",
    "        line_width=0\n",
    "    ))\n",
    "\n",
    "    # Añadimos los puntos de datos\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X[:, 0], y=X[:, 1], mode='markers',\n",
    "        marker=dict(color=y, colorscale=['blue', 'red'], line=dict(width=1, color='DarkSlateGrey')),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    # Calculamos la precisión (accuracy)\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "\n",
    "    fig.update_layout(title=f'{title}<br>Accuracy: {acc:.4f}',\n",
    "                      xaxis_title='Característica 1 (escalada)',\n",
    "                      yaxis_title='Característica 2 (escalada)',\n",
    "                      height=600, width=800,)\n",
    "    return fig"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1. Regresión Logística (Modelo Base)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "fig = plot_decision_boundary(log_reg, X_train, y_train, 'Regresión Logística')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar, la Regresión Logística intenta separar los datos con una línea recta, fallando en capturar la estructura curva de las \"lunas\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. SVM con Kernel Lineal"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "fig = plot_decision_boundary(svm_linear, X_train, y_train, 'SVM con Kernel Lineal')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El SVM lineal produce un resultado muy similar a la Regresión Logística. Ambos son modelos lineales y, por lo tanto, inadecuados para este problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. SVM con Kernel Polinómico"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "svm_poly = SVC(kernel='poly', degree=3, random_state=42)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "fig = plot_decision_boundary(svm_poly, X_train, y_train, 'SVM con Kernel Polinómico (grado 3)')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "¡Mucho mejor! El kernel polinómico puede crear una frontera curva que se adapta mucho mejor a la forma de los datos. La precisión mejora un poco."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. SVM con Kernel RBF (Radial Basis Function)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "fig = plot_decision_boundary(svm_rbf, X_train, y_train, 'SVM con Kernel RBF')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel RBF, que es el más flexible y popular, logra la mejor separación. Crea una frontera suave y compleja que se ajusta casi perfectamente a la estructura de los datos, logrando la mayor precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. El Impacto de los Hiperparámetros `C` y `gamma`\n",
    "\n",
    "En SVM, `C` y `gamma` (para el kernel RBF) son cruciales. Vamos a visualizar su efecto.\n",
    "\n",
    "- **`C` (Parámetro de regularización):** Controla el trade-off entre un margen ancho y clasificar correctamente todos los puntos. Un `C` bajo crea un margen más ancho pero permite más violaciones (más simple, más sesgo). Un `C` alto crea un margen más estrecho, intentando clasificar todo correctamente (más complejo, más varianza).\n",
    "- **`gamma`:** Define la influencia de un único ejemplo de entrenamiento. Un `gamma` bajo significa que un punto tiene una influencia lejana (frontera más suave). Un `gamma` alto significa que un punto tiene una influencia muy local (frontera puede ser muy irregular)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Creamos una figura con subplots\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\n",
    "    \"Bajo Gamma, Bajo C\", \"Alto Gamma, Bajo C\",\n",
    "    \"Bajo Gamma, Alto C\", \"Alto Gamma, Alto C\"\n",
    "))\n",
    "\n",
    "hyperparams = [(0.1, 1), (20, 1), (0.1, 2000), (20, 2000)]\n",
    "plot_positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "for (gamma, C), pos in zip(hyperparams, plot_positions):\n",
    "    # Entrenar modelo\n",
    "    svm = SVC(kernel='rbf', gamma=gamma, C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Crear malla\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Predecir en la malla\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    # Añadir trazas a la sub-figura\n",
    "    fig.add_trace(go.Contour(x=xx[0], y=yy[:, 0], z=Z, showscale=False, colorscale=['rgba(0,0,255,0.2)', 'rgba(255,0,0,0.2)'], line_width=0), row=pos[0], col=pos[1])\n",
    "    fig.add_trace(go.Scatter(x=X_train[:, 0], y=X_train[:, 1], mode='markers', marker=dict(color=y_train, colorscale=['blue', 'red'])), row=pos[0], col=pos[1])\n",
    "\n",
    "fig.update_layout(height=800, width=800, title_text=\"Efecto de Hiperparámetros C y Gamma en Kernel RBF\", showlegend=False)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Desafíos\n",
    "\n",
    "1.  **Evaluar en el conjunto de test:** Todos los `accuracy` se calcularon sobre los datos de entrenamiento para visualizar el ajuste. Calcula la precisión de los 4 modelos principales (LogReg, SVM Lineal, Polinómico, RBF con valores por defecto) en el conjunto de `X_test`, `y_test`. ¿Cuál generaliza mejor?\n",
    "2.  **Probar otros datasets:** Importa el dataset de `make_circles` de `sklearn.datasets`. ¿Cómo se comportan los diferentes kernels en este caso? ¿Y con `make_blobs` con 3 centros?\n",
    "3.  **Ajuste fino:** Usando los plots de la sección 4 como guía, intenta encontrar \"a mano\" una combinación de `C` y `gamma` que te parezca que da el mejor resultado (un buen balance entre ajuste y suavidad de la frontera)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ejercicios Propuestos\n",
    "\n",
    "1.  **Interpretación del Margen:** En el SVM con kernel lineal, ¿cómo podrías identificar visualmente cuáles son los vectores de soporte? (Pista: son los puntos más cercanos a la línea de separación).\n",
    "\n",
    "2.  **Kernel Polinómico y Grado:** Modifica el grado `degree` del `SVC(kernel='poly', ...)` a 1, 2, 5 y 10. Describe cómo cambia la complejidad de la frontera de decisión y el riesgo de sobreajuste (overfitting).\n",
    "\n",
    "3.  **El rol de `coef0`:** En el kernel polinómico, el parámetro `coef0` controla cuánto influyen los polinomios de grado alto vs. los de grado bajo. Investiga y experimenta con este parámetro. ¿Qué efecto tiene en la frontera de decisión?\n",
    "\n",
    "4.  **SVM para Regresión (SVR):** La librería `sklearn.svm` también contiene `SVR`. Investiga cómo funciona SVR. A diferencia de SVM para clasificación que intenta poner una \"calle\" vacía entre clases, SVR intenta que la mayor cantidad de puntos queden *dentro* de la \"calle\". Aplica SVR a un dataset de regresión simple.\n",
    "\n",
    "5.  **Coste Computacional:** Aumenta el número de muestras en `make_moons` a 5000 y luego a 20000. Mide el tiempo de entrenamiento de la Regresión Logística y del SVM con kernel RBF usando `%%timeit` en la celda. ¿Qué observas sobre la escalabilidad de SVM?\n",
    "\n",
    "6.  **Importancia del Escalado:** Vuelve a ejecutar el notebook pero esta vez, comenta las líneas donde se usa `StandardScaler`. ¿Qué impacto tiene en el rendimiento del SVM, especialmente con kernel RBF? ¿Por qué crees que ocurre esto? (Pista: el kernel RBF depende de distancias).\n",
    "\n",
    "7.  **Creando un Kernel Personalizado:** La clase `SVC` de Scikit-Learn permite pasar una función como kernel. Define una función en Python que reciba dos vectores `X1` y `X2` y devuelva su producto punto (kernel lineal). Úsala para entrenar un SVM: `SVC(kernel=mi_kernel_lineal)`. Obs: el parametro debe ser una \"callable\" que reciba dos arrays y devuelva un escalar. ¿Cómo se compara con el SVM lineal de Scikit-Learn?\n",
    "\n",
    "8.  **El parámetro `C` a fondo:** Crea un bucle que entrene varios SVM con kernel lineal pero con valores de `C` muy diferentes (ej. 0.01, 1, 100, 10000). Para cada uno, imprime el número de vectores de soporte (`model.n_support_`). ¿Qué relación observas entre el valor de `C` y la cantidad de vectores de soporte?\n",
    "\n",
    "9.  **Aplicación a un problema real:** Carga el dataset de cáncer de mama de sklearn (`load_breast_cancer`). Como es un dataset con muchas características, no podrás visualizar la frontera de decisión. En su lugar, usa `GridSearchCV` de `sklearn.model_selection` para encontrar la mejor combinación de `C` y `gamma` para un SVM con kernel RBF. Compara su `accuracy` en el conjunto de test con el de una Regresión Logística."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
