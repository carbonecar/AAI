{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEX29wNNQQiQ"
   },
   "source": [
    "# Taller Práctico: Regresión Logística (Versión Completa y Corregida)\n",
    "\n",
    "**Curso:** Maestría en Machine Learning\n",
    "\n",
    "**Nota:** Esta versión incorpora las correcciones de preprocesamiento discutidas: la correcta creación de variables dummy (0/1) y la estandarización de features numéricos, presentando el flujo de trabajo metodológicamente correcto y completo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtXka06yQQiS"
   },
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teOFJP_hQRTu"
   },
   "outputs": [],
   "source": [
    "!pip install ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BzWFrvFzQQiS"
   },
   "outputs": [],
   "source": [
    "# Librerías para manipulación de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías para visualización\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Librerías de Machine Learning (scikit-learn)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Librería para cargar el dataset\n",
    "from ISLP import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx1a4IQ7QQiT"
   },
   "source": [
    "## 2. Carga y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "35soQHJIQQiT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas de los datos preprocesados:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "      <th>student_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       balance        income  student_Yes\n",
       "0   729.526495  44361.625074            0\n",
       "1   817.180407  12106.134700            1\n",
       "2  1073.549164  31767.138947            0\n",
       "3   529.250605  35704.493935            0\n",
       "4   785.655883  38463.495879            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carga de datos\n",
    "default_df = load_data('Default')\n",
    "\n",
    "# Conversión de categóricas a numéricas\n",
    "df_processed = pd.get_dummies(default_df, columns=['student'], drop_first=True, dtype=int)\n",
    "df_processed['default'] = df_processed['default'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# Definición de features y target\n",
    "X = df_processed[['balance', 'income', 'student_Yes']]\n",
    "y = df_processed['default']\n",
    "\n",
    "print(\"Primeras filas de los datos preprocesados:\")\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s0jwOTsQQiU"
   },
   "source": [
    "### División en Conjuntos de Entrenamiento y Prueba\n",
    "\n",
    "Dividimos los datos ANTES de cualquier preprocesamiento que \"aprenda\" de los datos, como la estandarización, para evitar fuga de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eGhZA41sQQiU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (8000, 3)\n",
      "Forma de X_test: (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMwv7uO-QQiU"
   },
   "source": [
    "### Estandarización de Features Numéricos\n",
    "\n",
    "**Este es un paso crucial.** Estandarizamos las variables `balance` e `income` para que tengan media 0 y desviación estándar 1. Esto asegura que la regularización del modelo se aplique de manera justa.\n",
    "\n",
    "**Importante:** Ajustamos el `StandardScaler` **únicamente** con los datos de entrenamiento (`X_train`) y luego lo usamos para transformar tanto el conjunto de entrenamiento como el de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F92uEGLTQQiV"
   },
   "outputs": [],
   "source": [
    "# Instanciamos el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Columnas a escalar\n",
    "cols_to_scale = ['balance', 'income']\n",
    "\n",
    "# Creamos copias para no modificar los dataframes originales\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Ajustamos y transformamos los datos de entrenamiento\n",
    "X_train_scaled[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
    "\n",
    "# Transformamos los datos de prueba con el escalador ya ajustado\n",
    "X_test_scaled[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "\n",
    "print(\"Primeras filas de los datos de entrenamiento escalados:\")\n",
    "display(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4vON1LBQQiV"
   },
   "source": [
    "## 3. Entrenamiento del Modelo de Regresión Logística\n",
    "\n",
    "Ahora entrenaremos el modelo usando los datos estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsDmZG1RQQiW"
   },
   "outputs": [],
   "source": [
    "# Instanciamos y entrenamos el modelo con los datos escalados\n",
    "log_reg = LogisticRegression(random_state=42)#, solver='liblinear')\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"¡Modelo entrenado exitosamente con datos escalados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl5Of2i0QQiW"
   },
   "source": [
    "## 4. Evaluación del Modelo\n",
    "\n",
    "Procedemos a la evaluación completa del modelo sobre el conjunto de prueba escalado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfC7MO6ZQQiW"
   },
   "source": [
    "### 4.1. Matriz de Confusión y Métricas Clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPnbLgxFQQiW"
   },
   "outputs": [],
   "source": [
    "# Realizamos predicciones sobre el conjunto de prueba escalado\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculamos y visualizamos la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Default', 'Default'])\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "disp.plot(cmap='Blues', ax=ax)\n",
    "ax.set_title('Matriz de Confusión')\n",
    "plt.show()\n",
    "\n",
    "# Imprimimos el reporte de clasificación completo\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Default', 'Default']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcNJ8WokQQiW"
   },
   "source": [
    "### 4.2. El Impacto del Umbral de Decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2Bc0-MaQQiW"
   },
   "outputs": [],
   "source": [
    "# Obtenemos las probabilidades de predicción para la clase positiva (Default)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Aplicamos un nuevo umbral de 0.2 para aumentar el Recall\n",
    "umbral = 0.2\n",
    "y_pred_nuevo_umbral = (y_pred_proba >= umbral).astype(int)\n",
    "\n",
    "print(f\"Reporte de Clasificación con Umbral = {umbral}:\")\n",
    "print(classification_report(y_test, y_pred_nuevo_umbral, target_names=['No Default', 'Default']))\n",
    "\n",
    "# Visualizamos la nueva matriz de confusión\n",
    "cm_nuevo = confusion_matrix(y_test, y_pred_nuevo_umbral)\n",
    "disp_nuevo = ConfusionMatrixDisplay(confusion_matrix=cm_nuevo, display_labels=['No Default', 'Default'])\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "disp_nuevo.plot(cmap='Greens', ax=ax)\n",
    "ax.set_title(f'Matriz de Confusión con Umbral = {umbral}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PsV6LgfQQiX"
   },
   "source": [
    "### 4.3. Curva ROC y AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xauP270QQiX"
   },
   "outputs": [],
   "source": [
    "# Calculamos la tasa de verdaderos positivos (tpr) y la tasa de falsos positivos (fpr)\n",
    "fpr, tpr, umbrales = roc_curve(y_test, y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Creamos el gráfico con Plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'Curva ROC (AUC = {auc:.4f})'))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Clasificador Aleatorio', line=dict(dash='dash')))\n",
    "fig.update_layout(\n",
    "    title='Curva ROC para el Modelo de Regresión Logística',\n",
    "    xaxis_title='Tasa de Falsos Positivos (FPR)',\n",
    "    yaxis_title='Tasa de Verdaderos Positivos (TPR)',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWqZ5lm5QQiX"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Umbral': umbrales,\n",
    "    'FPR': fpr,\n",
    "    'TPR': tpr\n",
    "})  # Mostramos las primeras filas de los umbrales y sus correspondientes FPR y TPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey7AUT45QQiX"
   },
   "source": [
    "## 5. Regularización\n",
    "\n",
    "El hiperparámetro `C` en `scikit-learn` es el inverso de la fuerza de regularización $\\lambda$. Un valor de `C` pequeño implica una regularización más fuerte. Ahora que los datos están escalados, la comparación de coeficientes es mucho más justa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpZrqn2cQQiX"
   },
   "outputs": [],
   "source": [
    "# Modelo con regularización L2 (Ridge) - C=1.0 (default)\n",
    "log_reg_l2 = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', random_state=42).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Modelo con regularización L1 (Lasso) - C=1.0 (default)\n",
    "log_reg_l1 = LogisticRegression(penalty='l1', C=1.0, solver='liblinear', random_state=42).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Ahora con regularización más fuerte (C=0.1)\n",
    "log_reg_l2_strong = LogisticRegression(penalty='l2', C=0.1, solver='liblinear', random_state=42).fit(X_train_scaled, y_train)\n",
    "log_reg_l1_strong = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', random_state=42).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Creamos un DataFrame para comparar los coeficientes\n",
    "coef_df = pd.DataFrame({\n",
    "    'Predictor': X.columns,\n",
    "    'Coef_L2 (C=1.0)': log_reg_l2.coef_[0],\n",
    "    'Coef_L1 (C=1.0)': log_reg_l1.coef_[0],\n",
    "    'Coef_L2_fuerte (C=0.1)': log_reg_l2_strong.coef_[0],\n",
    "    'Coef_L1_fuerte (C=0.1)': log_reg_l1_strong.coef_[0]\n",
    "})\n",
    "\n",
    "print(\"Comparación de Coeficientes con Regularización (Datos Escalados):\")\n",
    "display(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-os9B39dQQiX"
   },
   "source": [
    "**Análisis:**\n",
    "- Ahora los coeficientes son directamente comparables en magnitud.\n",
    "- Se puede observar claramente cómo al aumentar la fuerza de la regularización (disminuir `C`), los coeficientes se \"encogen\" hacia cero.\n",
    "- La regularización L1 es más agresiva y puede llevar coeficientes a cero, realizando una selección de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo5kcoNhQQiX"
   },
   "source": [
    "## 6. Preguntas y Desafíos para Experimentar\n",
    "\n",
    "Ahora te toca a ti. Intenta responder a las siguientes preguntas modificando el código anterior para solidificar tu aprendizaje:\n",
    "\n",
    "1.  **Fuerza de Regularización vs. Rendimiento:** Crea un gráfico que muestre cómo cambia el AUC del modelo a medida que varías el hiperparámetro `C` (p. ej. `[0.001, 0.01, 0.1, 1, 10, 100]`). ¿Encuentras un valor óptimo para `C`?\n",
    "2.  **Otros Escaladores:** En lugar de `StandardScaler`, prueba con `MinMaxScaler` de `sklearn.preprocessing`. ¿Cambian los resultados de rendimiento? ¿Y los coeficientes del modelo?\n",
    "3.  **Selección de Predictores:** Entrena el modelo usando solo `balance` y `student_Yes` (ambos escalados). ¿Cuánto cae el rendimiento (AUC) al quitar la variable `income`? ¿Confirma esto que `income` es el predictor menos importante?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzQ4eRD6QQiY"
   },
   "source": [
    "## 7. Ejercicios Propuestos\n",
    "\n",
    "1.  Explica con tus propias palabras por qué el accuracy no es una buena métrica para evaluar un modelo en un dataset desbalanceado como `Default`.\n",
    "2.  Si un banco te pide un modelo que minimice la cantidad de clientes en default que no son detectados, ¿en qué métrica te enfocarías (Precisión o Recall)? ¿Y cómo ajustarías el umbral de decisión?\n",
    "3.  Carga un nuevo dataset de clasificación de `scikit-learn` (por ejemplo, `load_breast_cancer`) y realiza un análisis completo: estandarización, entrenamiento, evaluación con matriz de confusión y reporte de clasificación.\n",
    "4.  ¿Qué representa un punto en la esquina superior izquierda de la curva ROC? ¿Y un punto en la línea diagonal?\n",
    "5.  ¿Por qué es fundamental ajustar (`fit`) el `StandardScaler` únicamente con los datos de entrenamiento y no con todo el dataset?\n",
    "6.  Crea un bucle `for` que itere sobre una lista de umbrales (ej. `[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]`). Para cada umbral, calcula y guarda la Precisión y el Recall para la clase 'Default'. Luego, grafica cómo cambian estas dos métricas a medida que varía el umbral (Precision-Recall Curve).\n",
    "7.  Usando los coeficientes del modelo `log_reg` (entrenado con datos escalados), interpreta el coeficiente de la variable `balance`. ¿Qué significa un aumento de una desviación estándar en el `balance` en términos del odds ratio de default?\n",
    "8.  Explica la diferencia fundamental entre la regularización L1 y L2 en términos de su efecto sobre los coeficientes del modelo.\n",
    "9.  Si tuvieras dos modelos, uno con AUC de 0.85 y otro con AUC de 0.75, ¿cuál elegirías y por qué? ¿Bajo qué circunstancias podrías preferir el de 0.75?\n",
    "10. Investiga el parámetro `class_weight` de `LogisticRegression` en `scikit-learn`. ¿Cómo podrías usarlo para manejar el desbalanceo de clases en el dataset `Default`? Pruébalo (con los datos escalados) y compara los resultados (recall, precisión) con la técnica de ajustar el umbral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHXsgaHMQQiY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
