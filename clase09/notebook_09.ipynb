{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller Práctico Clase 9: Reducción de Dimensionalidad\n",
    "\n",
    "**Objetivo:** En este taller, aplicaremos y compararemos las tres técnicas de reducción de dimensionalidad vistas en clase: PCA (Análisis de Componentes Principales), t-SNE y UMAP. \n",
    "\n",
    "**Dataset:** Usaremos el dataset `digits` de Scikit-learn, que contiene imágenes de 8x8 píxeles de dígitos escritos a mano (0-9). Cada imagen está \"aplanada\" en un vector de 64 características (dimensiones), lo que lo convierte en un excelente candidato para la reducción de dimensionalidad y la visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración e Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Librerías para manipulación de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Librerías para visualización\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn para carga de datos, preprocesamiento y modelos\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# UMAP (es necesario instalarlo: pip install umap-learn)\n",
    "import umap\n",
    "\n",
    "# Configuraciones adicionales\n",
    "pd.options.plotting.backend = 'plotly'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Exploración del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cargamos el dataset de dígitos\n",
    "digits = load_digits()\n",
    "\n",
    "# X tiene los datos de las imágenes (las características)\n",
    "X = digits.data\n",
    "\n",
    "# y contiene las etiquetas verdaderas (el dígito del 0 al 9)\n",
    "y = digits.target\n",
    "\n",
    "print(f\"Forma de la matriz de características (X): {X.shape}\")\n",
    "print(f\"Forma del vector de etiquetas (y): {y.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen tiene 64 píxeles (8x8), que actúan como nuestras características. Tenemos 1797 imágenes en total.\n",
    "\n",
    "Para tener una idea de cómo se ven los datos, podemos visualizar algunas de las imágenes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "digits.images[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "digits.data",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(7.5, 3))\n",
    "for index, (image, label) in enumerate(zip(digits.images[:10], digits.target[:10])):\n",
    "    plt.subplot(2, 5, index + 1)\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('Label: %i' % label)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento: Escalado de Datos\n",
    "\n",
    "Como vimos en la teoría, PCA es sensible a la escala de las características. Aunque en este caso todos los píxeles están en una escala similar (0-16), es una buena práctica estandarizar los datos para que tengan media 0 y desviación estándar 1. Esto asegura que cada característica contribuya por igual al análisis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis de Componentes Principales (PCA)\n",
    "\n",
    "Comenzaremos aplicando PCA para reducir la dimensionalidad de 64 a 2 componentes para poder visualizar los datos en un gráfico de dispersión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Varianza Explicada\n",
    "\n",
    "Antes de reducir a 2D, es útil ajustar PCA con todos los componentes posibles para ver cuánta varianza explica cada uno. El **Scree Plot** nos ayudará a decidir cuántos componentes son \"suficientes\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ajustamos PCA sin especificar n_components para mantenerlos todos\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Calculamos la varianza explicada acumulada\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Creamos el Scree Plot\n",
    "fig = px.area(x=range(1, cumulative_variance.shape[0] + 1),\n",
    "              y=cumulative_variance,\n",
    "              labels={\"x\": \"Número de Componentes Principales\", \"y\": \"Varianza Explicada Acumulada\"},\n",
    "              title=\"Scree Plot: Varianza Explicada por los Componentes Principales\")\n",
    "fig.update_layout(yaxis_range=[0,1.05])\n",
    "fig.add_hline(y=0.9, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.add_annotation(x=30, y=0.9, text=\"90% de Varianza Explicada\", showarrow=True, arrowhead=1)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación:** Podemos ver que con aproximadamente 10 componentes ya capturamos más del 60% de la varianza, y con unos 30 componentes, superamos el 90%. Esto nos dice que hay mucha redundancia en los 64 píxeles y que podemos representar los datos con mucha menos información sin una pérdida masiva.\n",
    "\n",
    "Ahora, vamos a reducir a solo 2 componentes para la visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Reducción a 2D y Visualización"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Aplicamos PCA para reducir a 2 dimensiones\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# Creamos un DataFrame para facilitar la visualización con Plotly\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['label'] = y.astype(str) # Convertimos la etiqueta a string para colores discretos\n",
    "\n",
    "# Visualizamos\n",
    "fig = px.scatter(df_pca, x='PC1', y='PC2', color='label',\n",
    "                 title='Visualización del Dataset Digits con PCA (2 Componentes)',\n",
    "                 labels={'color': 'Dígito'})\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis de PCA:** PCA logra una separación decente para algunos dígitos (como el 0, 4 y 6), pero muchos otros se superponen significativamente. Esto es esperado, ya que PCA está limitado a proyecciones lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "Ahora, aplicaremos t-SNE, una técnica no lineal, para ver si podemos obtener una mejor separación visual de los clusters de dígitos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Aplicamos t-SNE. Perplexity es un hiperparámetro importante.\n",
    "tsne = TSNE(n_components=2, perplexity=100, random_state=1)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Creamos el DataFrame para Plotly\n",
    "df_tsne = pd.DataFrame(data=X_tsne, columns=['TSNE1', 'TSNE2'])\n",
    "df_tsne['label'] = y.astype(str)\n",
    "\n",
    "# Visualizamos\n",
    "fig = px.scatter(df_tsne, x='TSNE1', y='TSNE2', color='label',\n",
    "                 title='Visualización del Dataset Digits con t-SNE',\n",
    "                 labels={'color': 'Dígito'})\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis de t-SNE:** ¡El resultado es drásticamente mejor! t-SNE logra formar clusters muy definidos y separados para cada dígito. Esto demuestra el poder de los métodos no lineales para visualizar estructuras de datos complejas. Recordemos la advertencia: las distancias entre los clusters en t-SNE no son necesariamente significativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "Finalmente, probaremos UMAP, la alternativa moderna a t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Aplicamos UMAP. n_neighbors y min_dist son hiperparámetros clave.\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.0, n_components=2)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "# Creamos el DataFrame para Plotly\n",
    "df_umap = pd.DataFrame(data=X_umap, columns=['UMAP1', 'UMAP2'])\n",
    "df_umap['label'] = y.astype(str)\n",
    "\n",
    "# Visualizamos\n",
    "fig = px.scatter(df_umap, x='UMAP1', y='UMAP2', color='label',\n",
    "                 title='Visualización del Dataset Digits con UMAP',\n",
    "                 labels={'color': 'Dígito'})\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Análisis de UMAP:** UMAP también produce una excelente separación de los clusters, similar a t-SNE. A menudo, UMAP es mucho más rápido que t-SNE (aunque en este dataset pequeño la diferencia no es tan notable) y puede preservar mejor la estructura global de los datos. Observa cómo algunos clusters (como el '4' y el '7') pueden aparecer más cercanos entre sí, lo que podría reflejar una mayor similitud intrínseca."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusiones del Taller\n",
    "\n",
    "- **PCA** es una herramienta rápida y útil para una reducción de dimensionalidad lineal. Es excelente como paso de preprocesamiento, pero puede no ser la mejor para la visualización de estructuras complejas.\n",
    "- **t-SNE** es excepcional para revelar la estructura de clusters locales en los datos, produciendo visualizaciones muy claras y separadas.\n",
    "- **UMAP** ofrece resultados de visualización de alta calidad, a menudo comparables o superiores a los de t-SNE, pero con la ventaja de ser significativamente más rápido y, a menudo, preservar mejor la estructura global de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✍️ Ejercicios Propuestos\n",
    "\n",
    "1.  **Varianza en PCA:** ¿Cuántos componentes principales son necesarios para explicar el 95% de la varianza en el dataset `digits`? Vuelve a ejecutar el análisis del Scree Plot y encuentra el número exacto.\n",
    "\n",
    "2.  **Experimenta con t-SNE:** Modifica el hiperparámetro `perplexity` en el modelo t-SNE a valores más bajos (ej. 5) y más altos (ej. 200). ¿Cómo cambia la visualización? ¿Qué parece controlar este parámetro?\n",
    "\n",
    "3.  **Experimenta con UMAP:** Modifica el hiperparámetro `n_neighbors` en el modelo UMAP a valores más bajos (ej. 5) y más altos (ej. 50). ¿Cómo afecta esto a la separación de los clusters y a la estructura global?\n",
    "\n",
    "4.  **Visualización 3D:** Adapta el código de PCA, t-SNE y UMAP para reducir los datos a 3 componentes (`n_components=3`). Utiliza `plotly.express.scatter_3d` para crear visualizaciones tridimensionales. ¿Añade la tercera dimensión una mejor perspectiva?\n",
    "\n",
    "5.  **Impacto en un Clasificador:** Entrena un modelo simple, como `LogisticRegression` de Scikit-learn, sobre los datos originales escalados (`X_scaled`). Luego, entrena el mismo modelo sobre los datos reducidos por PCA (usando suficientes componentes para capturar el 95% de la varianza). Compara el tiempo de entrenamiento y la precisión (accuracy) de ambos modelos. ¿Qué observas?\n",
    "\n",
    "6.  **Interpretando los Componentes Principales:** El objeto `pca_2d` contiene los `components_` (loadings). Cada componente es un vector de 64 elementos. \"Reforma\" (`reshape`) el primer componente principal a una imagen de 8x8 y visualízalo con `plt.imshow`. ¿Qué parece representar este componente? ¿Qué píxeles son los más importantes?\n",
    "\n",
    "7.  **Aplicación a otro Dataset:** Carga el dataset `load_wine` de Scikit-learn. Este dataset tiene 13 características. Aplica PCA, t-SNE y UMAP para visualizar las 3 clases de vinos. ¿Cuál técnica funciona mejor para separar las clases?\n",
    "le? ¿Por qué?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Observación sobre el punto 5:**\n",
    "En mi PC observé los siguientes datos de accuracy y tiempo, como para darles una referencia:\n",
    "\n",
    "| Métrica                     | Modelo Original | Modelo con PCA |\n",
    "|-----------------------------|----------------|---------------|\n",
    "| Número de Características   | 64             | 39            |\n",
    "| Tiempo de Entrenamiento     | 0.0340         | 0.0062        |\n",
    "| Precisión (Accuracy)        | 0.9796         | 0.9630        |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
