{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 6: Taller Práctico - Random Forest y Boosting\n",
    "\n",
    "**Objetivos del Taller:**\n",
    "\n",
    "1.  Aplicar los algoritmos de Random Forest y Gradient Boosting a un problema de clasificación.\n",
    "2.  Entender cómo extraer y visualizar la importancia de las variables para interpretar los modelos.\n",
    "3.  Comparar el rendimiento y las características de ambos métodos de ensamble.\n",
    "4.  Experimentar con los hiperparámetros clave de cada modelo.\n",
    "\n",
    "Utilizaremos el dataset `Heart`, que contiene datos de pacientes para predecir la presencia de enfermedades cardíacas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- 1. Importación de Librerías --- \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Clases de Scikit-Learn para modelos y evaluación\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Librería del libro para cargar el dataset\n",
    "from ISLP import load_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Exploración de Datos\n",
    "\n",
    "Cargaremos el dataset `Heart` desde la librería `ISLP`. Este dataset es ideal para un problema de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL canónica del dataset Heart usado en el libro ISLP\n",
    "url = 'https://www.statlearning.com/s/Heart.csv'\n",
    "\n",
    "# Usamos pandas para leer el archivo CSV directamente desde la web\n",
    "heart_df = pd.read_csv(url)\n",
    "\n",
    "# A menudo, al cargar así, se crea una columna de índice no deseada. La eliminamos.\n",
    "if 'Unnamed: 0' in heart_df.columns:\n",
    "    heart_df = heart_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Verificamos que los datos se hayan cargado correctamente\n",
    "print('Datos cargados exitosamente desde la URL:')\n",
    "display(heart_df.head())\n",
    "\n",
    "print('\\\\nInformación del Dataset:')\n",
    "heart_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col_name in heart_df.select_dtypes(include=['object']).columns.tolist():\n",
    "    print(f\"Valores únicos en '{col_name}': {heart_df[col_name].unique()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Preparación de los Datos\n",
    "\n",
    "Para que los algoritmos de Scikit-learn funcionen correctamente, necesitamos realizar algunos pasos de preprocesamiento:\n",
    "\n",
    "1.  **Limpieza de Nulos:** Verificar y eliminar filas con valores nulos para simplificar el análisis.\n",
    "2.  **Codificación de Variables Categóricas:** Convertir columnas de texto (como `ChestPain`, `Thal`) en variables numéricas usando One-Hot Encoding.\n",
    "3.  **Separación de Datos:** Dividir el dataset en un conjunto de características (`X`) y una variable objetivo (`y`).\n",
    "4.  **División en Entrenamiento y Prueba:** Particionar los datos para entrenar el modelo y evaluarlo en datos no vistos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Limpieza de Nulos\n",
    "heart_df_clean = heart_df.dropna()\n",
    "\n",
    "# 2. Separación de características (X) y objetivo (y)\n",
    "X = heart_df_clean.drop('AHD', axis=1)\n",
    "y = heart_df_clean['AHD']\n",
    "\n",
    "# 3. Codificación One-Hot para variables categóricas\n",
    "# pd.get_dummies convierte columnas categóricas en 0s y 1s\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print('Columnas después de la codificación One-Hot:')\n",
    "print(X_encoded.columns)\n",
    "\n",
    "# 4. División en conjuntos de entrenamiento y prueba (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'\\nTamaño del set de entrenamiento: {X_train.shape[0]} muestras')\n",
    "print(f'Tamaño del set de prueba: {X_test.shape[0]} muestras')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest\n",
    "\n",
    "Ahora, entrenaremos nuestro primer modelo de ensamble. Un `RandomForestClassifier` es una colección de árboles de decisión entrenados en subconjuntos de datos y características, cuyas predicciones se combinan para obtener un resultado final más robusto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Instanciar el clasificador de Random Forest\n",
    "# n_estimators: número de árboles en el bosque.\n",
    "# random_state: para reproducibilidad de los resultados.\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=100, oob_score=True)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento\n",
    "print(\"--- Evaluación de Random Forest ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Error OOB (Out-of-Bag): {1 - rf_model.oob_score_:.4f}\") # Error OOB\n",
    "print(f\"Accuracy OOB: {rf_model.oob_score_:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Importancia de Variables\n",
    "Una de las grandes ventajas de Random Forest es que nos permite medir qué tan importante fue cada variable para la predicción. Lo visualizaremos para entender mejor qué factores influyen más en el diagnóstico."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extraer la importancia de las variables\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_encoded.columns\n",
    "\n",
    "# Crear un DataFrame para facilitar la visualización\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizar con Plotly Express\n",
    "fig = px.bar(\n",
    "    importance_df.head(15), \n",
    "    x='Importance', \n",
    "    y='Feature', \n",
    "    orientation='h', \n",
    "    title='Importancia de las Variables (Random Forest)',\n",
    "    labels={'Feature': 'Variable', 'Importance': 'Importancia'}\n",
    ")\n",
    "fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Mapeo de colores\n",
    "color_map = {\"Yes\": \"blue\", \"No\": \"red\"}\n",
    "ca_values = [0, 1, 2, 3]\n",
    "titles = [f'Ca = {val}' for val in ca_values]\n",
    "positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=titles)\n",
    "\n",
    "for idx, ca_val in enumerate(ca_values):\n",
    "    row, col = positions[idx]\n",
    "    df_plot = heart_df_clean[heart_df_clean[\"Ca\"] == ca_val]\n",
    "    show_legend = (idx == 0)\n",
    "    for ahd_val in [\"Yes\", \"No\"]:\n",
    "        df_class = df_plot[df_plot[\"AHD\"] == ahd_val]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_class[\"Oldpeak\"], y=df_class[\"MaxHR\"], mode=\"markers\",\n",
    "                marker=dict(color=color_map[ahd_val]), name=ahd_val, showlegend=show_legend,\n",
    "            ), row=row, col=col\n",
    "        )\n",
    "    show_legend = False\n",
    "\n",
    "# Set axis titles for all subplots\n",
    "for r in [1, 2]:\n",
    "    for c in [1, 2]:\n",
    "        fig.update_xaxes(title_text=\"Oldpeak\", row=r, col=c)\n",
    "        fig.update_yaxes(title_text=\"MaxHR\", row=r, col=c)\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=900,\n",
    "    title_text=\"Scatter Plots by Ca Value\",\n",
    "    legend=dict(title=\"AHD\")\n",
    ")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❔ Preguntas para Reflexionar (Random Forest)\n",
    "\n",
    "1.  **Experimenta con `n_estimators`**: ¿Cómo cambia el rendimiento y el error OOB si usas 10, 50, 200, o 500 árboles? ¿Hay un punto a partir del cual el rendimiento deja de mejorar significativamente?\n",
    "2.  **Analiza `max_features`**: Prueba cambiar este hiperparámetro. El valor por defecto es `sqrt(p)`. ¿Qué ocurre si lo ajustas a `None` (equivalente a Bagging) o a un número muy pequeño como `2`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Boosting\n",
    "\n",
    "Ahora, probaremos con Gradient Boosting. A diferencia de Random Forest, este método construye los árboles de forma secuencial, donde cada nuevo árbol intenta corregir los errores de los anteriores. Es conocido por su alto rendimiento, aunque puede ser más sensible a los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Instanciar el clasificador de Gradient Boosting\n",
    "# n_estimators: número de árboles secuenciales.\n",
    "# learning_rate: factor de contracción (shrinkage) para cada árbol.\n",
    "# max_depth: profundidad máxima de cada árbol (suelen ser superficiales).\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento\n",
    "print(\"--- Evaluación de Gradient Boosting ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Comparación de Matrices de Confusión\n",
    "\n",
    "La precisión general es útil, pero una matriz de confusión nos da más detalles sobre el tipo de errores que comete cada modelo (Falsos Positivos vs. Falsos Negativos), lo cual es crucial en problemas médicos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calcular matrices de confusión\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Matriz de Confusión - Random Forest')\n",
    "axes[0].set_xlabel('Predicho')\n",
    "axes[0].set_ylabel('Verdadero')\n",
    "\n",
    "sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('Matriz de Confusión - Gradient Boosting')\n",
    "axes[1].set_xlabel('Predicho')\n",
    "axes[1].set_ylabel('Verdadero')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Calcular ROC y AUC para ambos modelos\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test.map({\"No\": 0, \"Yes\": 1}), rf_model.predict_proba(X_test)[:, 1])\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "fpr_gb, tpr_gb, thresholds_gb = roc_curve(y_test.map({\"No\": 0, \"Yes\": 1}), gb_model.predict_proba(X_test)[:, 1])\n",
    "roc_auc_gb = auc(fpr_gb, tpr_gb)\n",
    "\n",
    "roc_df = pd.DataFrame({\n",
    "    \"FPR\": list(fpr_rf) + list(fpr_gb), \"TPR\": list(tpr_rf) + list(tpr_gb),\n",
    "    \"Threshold\": list(thresholds_rf) + list(thresholds_gb),\n",
    "    \"Model\": ([\"Random Forest\"] * len(fpr_rf)) + ([\"Gradient Boosting\"] * len(fpr_gb))\n",
    "})\n",
    "fig = px.line(\n",
    "    roc_df, x=\"FPR\", y=\"TPR\", color=\"Model\", line_dash=\"Model\", title=\"ROC Curves\",\n",
    "    labels={\"FPR\": \"False Positive Rate (FPR)\", \"TPR\": \"True Positive Rate (TPR)\"}, hover_data=[\"Threshold\"]\n",
    ")\n",
    "fig.add_shape(\n",
    "    type=\"line\", x0=0, y0=0, x1=1, y1=1,\n",
    "    line=dict(color=\"red\", dash=\"dash\")\n",
    ")\n",
    "fig.for_each_trace(\n",
    "    lambda t: t.update(\n",
    "        name=f\"{t.name} (AUC = {roc_auc_rf:.2f})\" if \"Random Forest\" in t.name else f\"{t.name} (AUC = {roc_auc_gb:.2f})\"\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=800, height=500,legend=dict(\n",
    "        x=0.98, y=0.02, xanchor=\"right\", yanchor=\"bottom\",\n",
    "        bgcolor=\"rgba(255,255,255,0.7)\", bordercolor=\"black\", borderwidth=1)\n",
    ")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❔ Preguntas para Reflexionar (Gradient Boosting)\n",
    "\n",
    "1.  **El trade-off `learning_rate` y `n_estimators`**: Reduce el `learning_rate` a `0.01`. ¿Cómo afecta esto a la precisión? ¿Necesitas más `n_estimators` para compensar? ¿Qué pasa si aumentas el `learning_rate` a `0.5`?\n",
    "2.  **Importancia de `max_depth`**: En Boosting, los árboles suelen ser superficiales. Prueba con `max_depth=1` (stumps). ¿Cómo se compara el resultado con `max_depth=3`?\n",
    "3.  **Análisis de errores**: Basándote en las matrices de confusión, ¿qué modelo es mejor para minimizar los **falsos negativos** (predecir 'No' cuando en realidad es 'Sí')? ¿Por qué esto podría ser lo más importante en un contexto de diagnóstico médico?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ejercicios Adicionales\n",
    "\n",
    "¡Es tu turno de explorar! A continuación, se proponen 10 ejercicios para profundizar en los conceptos vistos y experimentar por tu cuenta.\n",
    "\n",
    "1.  **Optimización de Profundidad:** Para ambos modelos (RF y GB), varía el hiperparámetro `max_depth` (ej. 2, 3, 5, 10) y analiza cómo afecta a la precisión en el conjunto de prueba. ¿Observas sobreajuste en alguno de los modelos cuando la profundidad es muy alta?\n",
    "\n",
    "2.  **Búsqueda de Hiperparámetros con `GridSearchCV`:** Utiliza `GridSearchCV` de Scikit-learn para encontrar la combinación óptima de hiperparámetros para el `RandomForestClassifier`. Busca sobre `n_estimators`, `max_depth` y `max_features`.\n",
    "\n",
    "3.  **Visualizar el Error OOB:** En el modelo de Random Forest, el atributo `oob_score_` se calcula sobre el ensamble final. Para visualizar la curva, entrena varios modelos con diferente `n_estimators` (de 10 a 200, por ejemplo) y grafica el error OOB (`1 - oob_score_`) en función del número de árboles.\n",
    "\n",
    "4.  **Probar un Dataset Diferente:** Carga el dataset de cáncer de mama de Scikit-learn (`from sklearn.datasets import load_breast_cancer`). Aplica tanto Random Forest como Gradient Boosting y compara sus resultados en este nuevo problema.\n",
    "\n",
    "5.  **Implementar `AdaBoost`:** Scikit-learn también tiene `AdaBoostClassifier`. Impleméntalo en el dataset `Heart` y compara su rendimiento y sus hiperparámetros con los de `GradientBoostingClassifier`.\n",
    "\n",
    "6.  **Curva de Aprendizaje en Boosting:** Para el modelo de Gradient Boosting, puedes acceder al error en cada etapa de la construcción secuencial. Utiliza el método `staged_predict()` para obtener las predicciones en cada iteración y grafica el error de prueba en función del número de árboles. ¿Observas un punto donde el error deja de bajar y empieza a subir (sobreajuste)?\n",
    "\n",
    "7.  **Importancia de Variables en Boosting:** Al igual que con Random Forest, los modelos de Gradient Boosting también tienen el atributo `.feature_importances_`. Extrae y visualiza la importancia de las variables para el modelo `gb_model`. ¿Coinciden las variables más importantes con las de Random Forest?\n",
    "\n",
    "8.  **Análisis de Probabilidades:** En lugar de predecir la clase directamente (`.predict()`), utiliza `.predict_proba()` para obtener las probabilidades. ¿Cómo podrías ajustar el umbral de decisión (por defecto es 0.5) para, por ejemplo, ser más sensible y capturar más casos positivos, aunque aumenten los falsos positivos?\n",
    "\n",
    "9.  **Investigar XGBoost:** Lee la documentación de la popular librería `xgboost`. Instálala (`pip install xgboost`) e impleméntala en este mismo problema. Compara su rendimiento y velocidad con el `GradientBoostingClassifier` de Scikit-learn.\n",
    "\n",
    "10. **Conclusión Final:** Escribe un párrafo en una celda de Markdown resumiendo tus hallazgos. ¿Qué modelo funcionó mejor para este problema? ¿Cuál recomendarías para un despliegue en producción y por qué, considerando la precisión, la interpretabilidad y el coste computacional?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
